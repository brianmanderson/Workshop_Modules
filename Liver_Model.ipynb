{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Liver Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import some things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def return_sys_path():\n",
    "    path = '.'\n",
    "    for _ in range(5):\n",
    "        if 'Deep_Learning' in os.listdir(path):\n",
    "            break\n",
    "        else:\n",
    "            path = os.path.join(path,'..')\n",
    "    return path\n",
    "def return_data_path():\n",
    "    path = '.'\n",
    "    for _ in range(5):\n",
    "        if 'Data' in os.listdir(path):\n",
    "            break\n",
    "        else:\n",
    "            path = os.path.join(path,'..')\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0217 17:43:22.398360 17780 deprecation.py:506] From c:\\users\\bmanderson\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.path.append(return_sys_path())\n",
    "from Deep_Learning.Base_Deeplearning_Code.Data_Generators.Generators import Data_Generator_Class, os\n",
    "from Deep_Learning.Base_Deeplearning_Code.Keras_Utils.Keras_Utilities import np, dice_coef_3D\n",
    "from Deep_Learning.Base_Deeplearning_Code.Data_Generators.Image_Processors import *\n",
    "from Deep_Learning.Base_Deeplearning_Code.Callbacks.Visualizing_Model_Utils import TensorBoardImage\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
    "# from Utils import ModelCheckpoint, model_path_maker\n",
    "from Deep_Learning.Base_Deeplearning_Code.Plot_And_Scroll_Images.Plot_Scroll_Images import plot_Image_Scroll_Bar_Image\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow.python.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = return_sys_path()\n",
    "data_path = os.path.join(return_data_path(),'Data','Niftii_Arrays')\n",
    "train_path = [os.path.join(data_path,'Train')]\n",
    "validation_path = os.path.join(data_path,'Validation')\n",
    "test_path = os.path.join(data_path,'Test')\n",
    "model_path = os.path.join(base,'Models')\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We now need some image processors...\n",
    "\n",
    "#### We will ensure that the images are 256 x 256 (downsampled for speed), normalize them with a mean of 78 and std of 29, add random noise, threshold, and turn into 2 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 128\n",
    "image_processors_train = [Ensure_Image_Proportions(image_size,image_size),Repeat_Channel(repeats=3),\n",
    "                          Normalize_Images(mean_val=78,std_val=29),\n",
    "                          Threshold_Images(lower_bound=-3.55,upper_bound=3.55, post_load=False),\n",
    "                          Annotations_To_Categorical(2)]\n",
    "#Add_Noise_To_Images(variation=np.round(np.arange(start=0, stop=0.3, step=0.1),2)),\n",
    "image_processors_test = [Ensure_Image_Proportions(image_size,image_size),Normalize_Images(mean_val=78,std_val=29),\n",
    "                         Repeat_Channel(repeats=3), Threshold_Images(lower_bound=-3.55,upper_bound=3.55),\n",
    "                         Annotations_To_Categorical(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "train_generator = Data_Generator_Class(by_patient=False,shuffle=True,data_paths=train_path,batch_size=batch_size,\n",
    "                                       image_processors=image_processors_train)\n",
    "validation_generator = Data_Generator_Class(by_patient=True,shuffle=True,data_paths=validation_path,batch_size=30,\n",
    "                                            image_processors=image_processors_test, by_patient_2D=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets visualize one of the examples! With batch_size of 5 and shuffle on, it will be 5 random 2D slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = train_generator.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "713e7c14ad49405eb07d1b881bf1bba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Z', max=4), Text(value='2D', description='view'), Outputâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_Image_Scroll_Bar_Image(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alright, lets make our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Deep_Learning.Easy_VGG16_UNet.Keras_Fine_Tune_VGG_16_Liver import VGG_16\n",
    "from Deep_Learning.Base_Deeplearning_Code.Visualizing_Model.Visualing_Model import visualization_model_class\n",
    "from tensorflow.python.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is just a click and play, it builds the VGG16 architecture for you with pre-trained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![VGG16_Unet.png](./Deep_Learning/Easy_VGG16_UNet/VGG16_UNet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n",
    "K.set_session(sess)\n",
    "network = {'Layer_0': {'Encoding': [64, 64], 'Decoding': [64]},\n",
    "           'Layer_1': {'Encoding': [128, 128], 'Decoding': [64]},\n",
    "           'Layer_2': {'Encoding': [256, 256, 256], 'Decoding': [256]},\n",
    "           'Layer_3': {'Encoding': [512, 512, 512], 'Decoding': [256]},\n",
    "           'Layer_4': {'Encoding': [512, 512, 512]}}\n",
    "VGG_model = VGG_16(network=network, activation='relu',filter_size=(3,3))\n",
    "VGG_model.make_model()\n",
    "VGG_model.load_weights()\n",
    "new_model = VGG_model.created_model\n",
    "model_path = os.path.join(return_sys_path(),'Models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## These are some tools for visualizing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Visualizing_Class = visualization_model_class(model=new_model, save_images=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets look at the activations of block1_conv1, the activation, and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualizing_Class.define_desired_layers(['block1_conv1','block1_conv1_activation','Output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualizing_Class.predict_on_tensor(x[0,...][None,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Visualizing_Class.plot_activations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_model.compile(Adam(lr=5e-5),loss='categorical_crossentropy', metrics=['accuracy',dice_coef_3D])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freezing pre-trained layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_until_name(model,name):\n",
    "    set_trainable = False\n",
    "    for layer in model.layers:\n",
    "        if layer.name == name:\n",
    "            set_trainable = True\n",
    "        layer.trainable = set_trainable\n",
    "    return model\n",
    "new_model = freeze_until_name(new_model,'Upsampling0_UNet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint and run\n",
    "\n",
    "A checkpoint is a way of assessing the model and determining if we should save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'VGG_16_Model'\n",
    "other_aspects = [model_name,'Upsampling0_UNet_Unfrozen'] # Just a list of defining things\n",
    "model_path_out = os.path.join(model_path,'VGG_16_frozen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(os.path.join(model_path_out,'best-model.hdf5'), monitor='val_dice_coef_3D', verbose=1, save_best_only=True,\n",
    "                              save_weights_only=False, mode='max')\n",
    "# TensorboardImage lets us view the predictions of our model\n",
    "tensorboard = TensorBoardImage(log_dir=model_path_out, num_images=3,update_freq='epoch', \n",
    "                               data_generator=validation_generator)\n",
    "callbacks = [checkpoint, tensorboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir {\"../Models\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.fit_generator(train_generator,epochs=5, workers=20, max_queue_size=50,\n",
    "                        validation_data=validation_generator,callbacks=callbacks,\n",
    "                        steps_per_epoch=200, image_frequency=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = validation_generator.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = new_model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[pred<0.5] = 0\n",
    "pred[pred>0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scroll_Image(pred[...,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now lets make our own architecture\n",
    "\n",
    "### First, lets import some necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Deep_Learning.Base_Deeplearning_Code.Models.Keras_3D_Models import my_3D_UNet\n",
    "from Deep_Learning.Base_Deeplearning_Code.Cyclical_Learning_Rate.clr_callback import CyclicLR\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
    "from functools import partial\n",
    "from tensorflow.python.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our convolution and strided blocks, strided is used for downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = 'relu'\n",
    "kernel = (3,3)\n",
    "pool_size = (2,2)\n",
    "#{'channels': x, 'kernel': (3, 3), 'strides': (1, 1),'activation':activation}\n",
    "conv_block = lambda x: {'channels': x}\n",
    "strided_block = lambda x: {'channels': x, 'strides': (2, 2)}\n",
    "pooling_downsampling = {'pooling': {'pooling_type': 'Max',\n",
    "                                    'pool_size': (2, 2), 'direction': 'Down'}}\n",
    "pooling_upsampling = {'pooling': {'pool_size': (2, 2), 'direction': 'Up'}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our architecture will have 2 main parts in each layer, an 'Encoding' and a 'Decoding' side, 'Encoding' goes down, and 'Decoding' goes up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Encoding and Decoding.png](./Deep_Learning/Encoding_and_Decoding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will now create our layer dictionary, this tells our UNet what to look like\n",
    "\n",
    "### If Pooling is left {} it will perform maxpooling and upsampling with pooling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dict = {}\n",
    "filters = 16\n",
    "layers_dict['Layer_0'] = {'Encoding':[conv_block(filters),conv_block(filters)],\n",
    "                          'Decoding':[conv_block(filters),conv_block(filters)],\n",
    "                          'Pooling':{'Encoding':pooling_downsampling,\n",
    "                                     'Decoding':pooling_upsampling\n",
    "                                    }}\n",
    "filters = 32\n",
    "layers_dict['Base'] = {'Encoding':[conv_block(filters), conv_block(filters),conv_block(filters)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Layer_0': {'Encoding': [{'channels': 16}, {'channels': 16}],\n",
       "  'Decoding': [{'channels': 16}, {'channels': 16}],\n",
       "  'Pooling': {}},\n",
       " 'Base': {'Encoding': [{'channels': 32}, {'channels': 32}, {'channels': 32}]}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer_0\n",
      "Base\n",
      "Layer_0\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n",
    "K.set_session(sess)\n",
    "new_model = my_3D_UNet(kernel=kernel,layers_dict=layers_dict, pool_size=pool_size,activation=activation,is_2D=True,\n",
    "                       input_size=3, image_size=image_size).created_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set a learning rate and loss metric, also add any metrics you want to track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_lr = 5e-6\n",
    "max_lr = 8e-4\n",
    "new_model.compile(Adam(lr=min_lr),loss='categorical_crossentropy', metrics=['accuracy',dice_coef_3D])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name your model and define other things! Send a list of strings and it will make a folder path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_path_maker' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-a598a27ce503>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m other_aspects = [model_name,'{}_Layers'.format(3),'{}_Conv_Blocks'.format(2),\n\u001b[0;32m      3\u001b[0m                  '{}_Filters'.format(16),'{}_MinLR_{}_MaxLR'.format(min_lr,max_lr)] # Just a list of defining things\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel_path_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_path_maker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mother_aspects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model_path_maker' is not defined"
     ]
    }
   ],
   "source": [
    "model_name = 'My_New_Model'\n",
    "other_aspects = [model_name,'{}_Layers'.format(3),'{}_Conv_Blocks'.format(2),\n",
    "                 '{}_Filters'.format(16),'{}_MinLR_{}_MaxLR'.format(min_lr,max_lr)] # Just a list of defining things\n",
    "model_path_out = model_path_maker(model_path,other_aspects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few more parameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = len(train_generator)//3\n",
    "step_size_factor = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is a checkpoint to save the model if it has the highest dice, also to add images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will specify that we want to watch the validation dice, and save the one with the highest value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor = 'val_dice_coef_3D'\n",
    "mode = 'max'\n",
    "checkpoint = ModelCheckpoint(os.path.join(model_path_out,'best-model.hdf5'), monitor=monitor, verbose=1, save_best_only=True,\n",
    "                             save_weights_only=False, save_freq='epoch', mode=mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, our tensorboard output will add prediction images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0217 17:45:50.838107 17780 Visualizing_Model_Utils.py:520] `batch_size` is no longer needed in the `TensorBoard` Callback and will be ignored in TensorFlow 2.0.\n"
     ]
    }
   ],
   "source": [
    "# TensorboardImage lets us view the predictions of our model\n",
    "tensorboard = TensorBoardImage(log_dir=os.path.join('.','Models'), batch_size=1, num_images=1,update_freq='epoch', \n",
    "                               data_generator=validation_generator, image_frequency=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CyclicLR will allow us to change the learning rate of the model as it runs, and Add_LR_To_Tensorboard will let us view it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyclic_lrate = CyclicLR(base_lr=min_lr, max_lr=max_lr, step_size=steps_per_epoch * step_size_factor, mode='triangular2')\n",
    "add_lr_to_tensorboard = Add_LR_To_Tensorboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [cyclic_lrate, add_lr_to_tensorboard, tensorboard, checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0217 17:46:03.330332 17780 deprecation.py:323] From c:\\users\\bmanderson\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278/279 [============================>.] - ETA: 0s - loss: 0.6448 - acc: 0.6834 - dice_coef_3D: 0.1135Epoch 1/10\n",
      "279/279 [==============================] - 124s 444ms/step - loss: 0.6445 - acc: 0.6844 - dice_coef_3D: 0.1133 - val_loss: 0.5456 - val_acc: 0.9282 - val_dice_coef_3D: 0.1011\n",
      "Epoch 2/10\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.4494 - acc: 0.9247 - dice_coef_3D: 0.1299Epoch 1/10\n",
      "279/279 [==============================] - 91s 327ms/step - loss: 0.4490 - acc: 0.9249 - dice_coef_3D: 0.1297 - val_loss: 0.3606 - val_acc: 0.9016 - val_dice_coef_3D: 0.1957\n",
      "Epoch 3/10\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.2387 - acc: 0.9347 - dice_coef_3D: 0.1843Epoch 1/10\n",
      "279/279 [==============================] - 82s 295ms/step - loss: 0.2384 - acc: 0.9347 - dice_coef_3D: 0.1845 - val_loss: 0.1918 - val_acc: 0.9103 - val_dice_coef_3D: 0.2922\n",
      "Epoch 4/10\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.1462 - acc: 0.9383 - dice_coef_3D: 0.2456Epoch 1/10\n",
      "279/279 [==============================] - 77s 277ms/step - loss: 0.1460 - acc: 0.9384 - dice_coef_3D: 0.2452 - val_loss: 0.1757 - val_acc: 0.8943 - val_dice_coef_3D: 0.3434\n",
      "Epoch 5/10\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.1266 - acc: 0.9365 - dice_coef_3D: 0.2935Epoch 1/10\n",
      "279/279 [==============================] - 76s 273ms/step - loss: 0.1266 - acc: 0.9365 - dice_coef_3D: 0.2937 - val_loss: 0.1456 - val_acc: 0.9151 - val_dice_coef_3D: 0.3360\n",
      "Epoch 6/10\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.1166 - acc: 0.9377 - dice_coef_3D: 0.3323Epoch 1/10\n",
      "279/279 [==============================] - 78s 278ms/step - loss: 0.1166 - acc: 0.9377 - dice_coef_3D: 0.3322 - val_loss: 0.1640 - val_acc: 0.9068 - val_dice_coef_3D: 0.3472\n",
      "Epoch 7/10\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.1071 - acc: 0.9437 - dice_coef_3D: 0.3549Epoch 1/10\n",
      "279/279 [==============================] - 76s 274ms/step - loss: 0.1070 - acc: 0.9438 - dice_coef_3D: 0.3547 - val_loss: 0.1158 - val_acc: 0.9426 - val_dice_coef_3D: 0.3208\n",
      "Epoch 8/10\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.1056 - acc: 0.9450 - dice_coef_3D: 0.3933Epoch 1/10\n",
      "279/279 [==============================] - 77s 276ms/step - loss: 0.1057 - acc: 0.9450 - dice_coef_3D: 0.3933 - val_loss: 0.1196 - val_acc: 0.9457 - val_dice_coef_3D: 0.3035\n",
      "Epoch 9/10\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.1019 - acc: 0.9503 - dice_coef_3D: 0.4195Epoch 1/10\n",
      "279/279 [==============================] - 76s 274ms/step - loss: 0.1018 - acc: 0.9503 - dice_coef_3D: 0.4193 - val_loss: 0.1372 - val_acc: 0.9303 - val_dice_coef_3D: 0.4522\n",
      "Epoch 10/10\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.0935 - acc: 0.9580 - dice_coef_3D: 0.4352Epoch 1/10\n",
      "279/279 [==============================] - 77s 278ms/step - loss: 0.0934 - acc: 0.9581 - dice_coef_3D: 0.4352 - val_loss: 0.0851 - val_acc: 0.9682 - val_dice_coef_3D: 0.3873\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21389e2d5f8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.fit_generator(train_generator,epochs=10, workers=10, max_queue_size=200, validation_data=validation_generator,\n",
    "                       callbacks=callbacks, steps_per_epoch=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir {\"./Models\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
