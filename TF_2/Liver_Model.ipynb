{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Liver Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import some things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def return_sys_path():\n",
    "    path = '.'\n",
    "    for _ in range(5):\n",
    "        if 'Deep_Learning' in os.listdir(path):\n",
    "            break\n",
    "        else:\n",
    "            path = os.path.join(path,'..')\n",
    "    return path\n",
    "def return_data_path():\n",
    "    path = '.'\n",
    "    for _ in range(5):\n",
    "        if 'Data' in os.listdir(path):\n",
    "            break\n",
    "        else:\n",
    "            path = os.path.join(path,'..')\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(return_sys_path())\n",
    "from Deep_Learning.Base_Deeplearning_Code.Data_Generators.TFRecord_to_Dataset_Generator import *\n",
    "from Deep_Learning.Base_Deeplearning_Code.Callbacks.TF2_Callbacks import Add_Images_and_LR\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from Deep_Learning.Base_Deeplearning_Code.Plot_And_Scroll_Images.Plot_Scroll_Images import plot_Image_Scroll_Bar_Image\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "base = return_sys_path()\n",
    "data_path = os.path.join(return_data_path(),'Data','Niftii_Arrays','Records')\n",
    "train_path = [os.path.join(data_path,'Train')]\n",
    "validation_path = [os.path.join(data_path,'Validation')]\n",
    "test_path = os.path.join(data_path,'Test')\n",
    "model_path = os.path.join(base,'Models')\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We now need some image processors...\n",
    "\n",
    "#### We will ensure that the images are 256 x 256 (downsampled for speed), normalize them with a mean of 78 and std of 29, add random noise, threshold, and turn into 2 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "image_size = 128\n",
    "wanted_keys={'inputs':['image'],'outputs':['annotation']}\n",
    "image_processors_train = [Expand_Dimensions(axis=-1),\n",
    "                          Ensure_Image_Proportions(image_size,image_size),\n",
    "                          Repeat_Channel(repeats=3),\n",
    "                          Normalize_Images(mean_val=78,std_val=29),\n",
    "                          Threshold_Images(lower_bound=-3.55,upper_bound=3.55),\n",
    "                          Return_Outputs(wanted_keys)]\n",
    "image_processors_validation = [Expand_Dimensions(axis=-1),\n",
    "                          Ensure_Image_Proportions(image_size,image_size),\n",
    "                          Repeat_Channel(repeats=3),\n",
    "                          Normalize_Images(mean_val=78,std_val=29),\n",
    "                          Threshold_Images(lower_bound=-3.55,upper_bound=3.55),\n",
    "                          Return_Outputs(wanted_keys)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Deep_Learning.Base_Deeplearning_Code.Data_Generators.Image_Processors_Module.Image_Processors_DataSet.Expand_Dimensions object at 0x000002EAC5ECFEF0>\n",
      "<Deep_Learning.Base_Deeplearning_Code.Data_Generators.Image_Processors_Module.Image_Processors_DataSet.Ensure_Image_Proportions object at 0x000002EAC5ECFF98>\n",
      "<Deep_Learning.Base_Deeplearning_Code.Data_Generators.Image_Processors_Module.Image_Processors_DataSet.Repeat_Channel object at 0x000002EAC5ECFF60>\n",
      "<Deep_Learning.Base_Deeplearning_Code.Data_Generators.Image_Processors_Module.Image_Processors_DataSet.Normalize_Images object at 0x000002EAC5ECFFD0>\n",
      "<Deep_Learning.Base_Deeplearning_Code.Data_Generators.Image_Processors_Module.Image_Processors_DataSet.Threshold_Images object at 0x000002EAC5EDC048>\n",
      "<Deep_Learning.Base_Deeplearning_Code.Data_Generators.Image_Processors_Module.Image_Processors_DataSet.Return_Outputs object at 0x000002EAC5EDC080>\n",
      "{'shuffle': 2484}\n",
      "{'batch': 5}\n",
      "{'repeat'}\n",
      "<Deep_Learning.Base_Deeplearning_Code.Data_Generators.Image_Processors_Module.Image_Processors_DataSet.Expand_Dimensions object at 0x000002EAC5ECFEB8>\n",
      "<Deep_Learning.Base_Deeplearning_Code.Data_Generators.Image_Processors_Module.Image_Processors_DataSet.Ensure_Image_Proportions object at 0x000002EAC5ECFF28>\n",
      "<Deep_Learning.Base_Deeplearning_Code.Data_Generators.Image_Processors_Module.Image_Processors_DataSet.Repeat_Channel object at 0x000002EAC5EDC128>\n",
      "<Deep_Learning.Base_Deeplearning_Code.Data_Generators.Image_Processors_Module.Image_Processors_DataSet.Normalize_Images object at 0x000002EAC5EDC160>\n",
      "<Deep_Learning.Base_Deeplearning_Code.Data_Generators.Image_Processors_Module.Image_Processors_DataSet.Threshold_Images object at 0x000002EAC5EDC198>\n",
      "<Deep_Learning.Base_Deeplearning_Code.Data_Generators.Image_Processors_Module.Image_Processors_DataSet.Return_Outputs object at 0x000002EAC5EDC1D0>\n",
      "{'repeat'}\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "train_generator = Data_Generator_Class(record_paths=train_path)\n",
    "validation_generator = Data_Generator_Class(record_paths=validation_path)\n",
    "image_processors_train += [\n",
    "            {'shuffle': len(train_generator)}, {'batch': batch_size}, {'repeat'}]\n",
    "image_processors_validation += [{'repeat'}]\n",
    "train_generator.compile_data_set(image_processors_train)\n",
    "validation_generator.compile_data_set(image_processors_validation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets visualize one of the examples! With batch_size of 5 and shuffle on, it will be 5 random 2D slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "x,y = next(iter(train_generator.data_set))\n",
    "x = x[0].numpy()\n",
    "y = y[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_Image_Scroll_Bar_Image(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_Image_Scroll_Bar_Image(np.argmax(y,axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alright, lets make our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from Deep_Learning.Easy_VGG16_UNet.Keras_Fine_Tune_VGG16_TF2 import VGG_16\n",
    "from Deep_Learning.Base_Deeplearning_Code.Visualizing_Model.Visualing_Model import visualization_model_class\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.compat.v1 import GPUOptions, ConfigProto, Session\n",
    "from tensorflow.python.keras.backend import set_session\n",
    "from Deep_Learning.Base_Deeplearning_Code.Callbacks.TF2_Callbacks import MeanDSC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is just a click and play, it builds the VGG16 architecture for you with pre-trained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![VGG16_Unet.png](./Deep_Learning/Easy_VGG16_UNet/VGG16_UNet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer_3\n",
      "Layer_2\n",
      "Layer_1\n",
      "Layer_0\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "gpu_options = GPUOptions(allow_growth=True)\n",
    "sess = Session(config=ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n",
    "set_session(sess)\n",
    "network = {'Layer_0': {'Encoding': [64, 64], 'Decoding': [64]},\n",
    "           'Layer_1': {'Encoding': [128, 128], 'Decoding': [64]},\n",
    "           'Layer_2': {'Encoding': [256, 256, 256], 'Decoding': [256]},\n",
    "           'Layer_3': {'Encoding': [512, 512, 512], 'Decoding': [256]},\n",
    "           'Layer_4': {'Encoding': [512, 512, 512]}}\n",
    "VGG_model = VGG_16(network=network, activation='relu',filter_size=(3,3))\n",
    "VGG_model.make_model()\n",
    "VGG_model.load_weights()\n",
    "new_model = VGG_model.created_model\n",
    "model_path = os.path.join(return_sys_path(),'Models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## These are some tools for visualizing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Visualizing_Class = visualization_model_class(model=new_model, save_images=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets look at the activations of block1_conv1, the activation, and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "Visualizing_Class.define_desired_layers(['block1_conv1','block1_conv1_activation','Output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "Visualizing_Class.predict_on_tensor(x[0,...][None,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Visualizing_Class.plot_activations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freezing pre-trained layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_model.compile(tf.keras.optimizers.Adam(5e-5), loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "                  metrics=[tf.keras.metrics.CategoricalAccuracy(), MeanDSC(num_classes=2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def freeze_until_name(model,name):\n",
    "    set_trainable = False\n",
    "    for layer in model.layers:\n",
    "        if layer.name == name:\n",
    "            set_trainable = True\n",
    "        layer.trainable = set_trainable\n",
    "    return model\n",
    "new_model = freeze_until_name(new_model,'Upsampling0_UNet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint and run\n",
    "\n",
    "A checkpoint is a way of assessing the model and determining if we should save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "model_name = 'VGG_16_Model'\n",
    "model_path_out = os.path.join(model_path,'VGG_16_frozen')\n",
    "if not os.path.exists(model_path_out):\n",
    "    os.makedirs(model_path_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(os.path.join(model_path_out,'cp-best.ckpt'), monitor='val_mean_dsc', verbose=1, save_best_only=True,\n",
    "                              save_weights_only=True, mode='max')\n",
    "\n",
    "add_images = Add_Images_and_LR(log_dir=model_path_out, validation_data=validation_generator.data_set,\n",
    "                               number_of_images=len(validation_generator), add_images=True, image_frequency=5,\n",
    "                               threshold_x=True, target_image_height=128, target_image_width=128)\n",
    "tensorboard = TensorBoard(log_dir=model_path_out)\n",
    "callbacks = [checkpoint, tensorboard, add_images]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets view the model real quick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "k = TensorBoard(model_path_out)\n",
    "k.set_model(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir {\"./Models\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "  2/496 [..............................] - ETA: 1:10 - loss: 0.3755 - categorical_accuracy: 0.8715 - mean_dsc: 0.1488WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.126997). Check your callbacks.\n",
      "496/496 [==============================] - ETA: 0s - loss: 0.0198 - categorical_accuracy: 0.9928 - mean_dsc: 0.9388- ETA\n",
      "Epoch 00001: val_mean_dsc improved from -inf to 0.96378, saving model to .\\..\\..\\Models\\VGG_16_frozen\\cp-best.ckpt\n",
      "WARNING:tensorflow:From c:\\users\\bmanderson\\virtual_environments\\tensorflow2.2\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: .\\..\\..\\Models\\VGG_16_frozen\\cp-best.ckpt\\assets\n",
      "Writing out images\n",
      "Writing out image 0\n",
      "Writing out image 1\n",
      "Writing out image 2\n",
      "496/496 [==============================] - 40s 81ms/step - loss: 0.0198 - categorical_accuracy: 0.9928 - mean_dsc: 0.9388 - val_loss: 0.0088 - val_categorical_accuracy: 0.9967 - val_mean_dsc: 0.9638\n",
      "Epoch 2/5\n",
      "496/496 [==============================] - ETA: 0s - loss: 0.0067 - categorical_accuracy: 0.9973 - mean_dsc: 0.9773\n",
      "Epoch 00002: val_mean_dsc improved from 0.96378 to 0.96567, saving model to .\\..\\..\\Models\\VGG_16_frozen\\cp-best.ckpt\n",
      "INFO:tensorflow:Assets written to: .\\..\\..\\Models\\VGG_16_frozen\\cp-best.ckpt\\assets\n",
      "496/496 [==============================] - 28s 56ms/step - loss: 0.0067 - categorical_accuracy: 0.9973 - mean_dsc: 0.9773 - val_loss: 0.0088 - val_categorical_accuracy: 0.9968 - val_mean_dsc: 0.9657\n",
      "Epoch 3/5\n",
      "496/496 [==============================] - ETA: 0s - loss: 0.0055 - categorical_accuracy: 0.9978 - mean_dsc: 0.9813\n",
      "Epoch 00003: val_mean_dsc did not improve from 0.96567\n",
      "496/496 [==============================] - 29s 58ms/step - loss: 0.0055 - categorical_accuracy: 0.9978 - mean_dsc: 0.9813 - val_loss: 0.0101 - val_categorical_accuracy: 0.9967 - val_mean_dsc: 0.9634\n",
      "Epoch 4/5\n",
      "496/496 [==============================] - ETA: 0s - loss: 0.0050 - categorical_accuracy: 0.9980 - mean_dsc: 0.9831- ETA: 3s - loss: 0.0050 - cate\n",
      "Epoch 00004: val_mean_dsc improved from 0.96567 to 0.96992, saving model to .\\..\\..\\Models\\VGG_16_frozen\\cp-best.ckpt\n",
      "INFO:tensorflow:Assets written to: .\\..\\..\\Models\\VGG_16_frozen\\cp-best.ckpt\\assets\n",
      "496/496 [==============================] - 32s 64ms/step - loss: 0.0050 - categorical_accuracy: 0.9980 - mean_dsc: 0.9831 - val_loss: 0.0076 - val_categorical_accuracy: 0.9972 - val_mean_dsc: 0.9699\n",
      "Epoch 5/5\n",
      "495/496 [============================>.] - ETA: 0s - loss: 0.0045 - categorical_accuracy: 0.9982 - mean_dsc: 0.9849\n",
      "Epoch 00005: val_mean_dsc did not improve from 0.96992\n",
      "496/496 [==============================] - 20s 39ms/step - loss: 0.0045 - categorical_accuracy: 0.9982 - mean_dsc: 0.9849 - val_loss: 0.0090 - val_categorical_accuracy: 0.9972 - val_mean_dsc: 0.9690\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2eafbf558d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.fit(train_generator.data_set, epochs=5, callbacks=callbacks, steps_per_epoch=len(train_generator),\n",
    "              validation_data=validation_generator.data_set, validation_steps=len(validation_generator),\n",
    "              validation_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "x,y = next(iter(validation_generator.data_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "pred = new_model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "pred[pred<0.5] = 0\n",
    "pred[pred>0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "plot_Image_Scroll_Bar_Image(pred[...,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now lets make our own architecture\n",
    "\n",
    "### First, lets import some necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from Deep_Learning.Base_Deeplearning_Code.Models.TF_Keras_Models import my_UNet, Return_Layer_Functions, return_hollow_layers_dict\n",
    "from Deep_Learning.Base_Deeplearning_Code.Cyclical_Learning_Rate.clr_callback_TF2 import CyclicLR\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our convolution and strided blocks, strided is used for downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "activation = {'activation': 'relu'}\n",
    "kernel = (3,3)\n",
    "pool_size = (2,2)\n",
    "#{'channels': x, 'kernel': (3, 3), 'strides': (1, 1),'activation':activation}\n",
    "conv_block = lambda x: {'convolution': {'channels': x, 'kernel': (3, 3),\n",
    "                                        'activation': None, 'strides': (1, 1)}}\n",
    "pooling_downsampling = {'pooling': {'pooling_type': 'Max',\n",
    "                                    'pool_size': (2, 2), 'direction': 'Down'}}\n",
    "pooling_upsampling = {'pooling': {'pool_size': (2, 2), 'direction': 'Up'}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our architecture will have 2 main parts in each layer, an 'Encoding' and a 'Decoding' side, 'Encoding' goes down, and 'Decoding' goes up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Encoding and Decoding.png](./Deep_Learning/Encoding_and_Decoding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will now create our layer dictionary, this tells our UNet what to look like\n",
    "\n",
    "### If Pooling is left {} it will perform maxpooling and upsampling with pooling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layers_dict(layers=1, filters=8, max_filters=np.inf, num_conv_blocks=2, num_classes=2,**kwargs):\n",
    "    lc = Return_Layer_Functions(kernel=(3,3),strides=(1,1),padding='same',batch_norm=True,\n",
    "                                pooling_type='Max', pool_size=(2,2), bn_before_activation=False)\n",
    "\n",
    "    block = lc.convolution_layer\n",
    "\n",
    "    layers_dict = return_hollow_layers_dict(layers)\n",
    "    pool = (2, 2)\n",
    "    final_filters = None\n",
    "    for layer in range(layers - 1):\n",
    "        layers_dict['Layer_' + str(layer)]['Encoding'] = []\n",
    "        encoding = []\n",
    "        for i in range(num_conv_blocks):\n",
    "            encoding += [block(filters)]\n",
    "        layers_dict['Layer_' + str(layer)]['Encoding'] += encoding\n",
    "        layers_dict['Layer_' + str(layer)]['Pooling']['Decoding'] = [lc.upsampling_layer(pool_size=pool),\n",
    "                                                                     lc.convolution_layer(filters)]\n",
    "        if filters < max_filters:\n",
    "            filters = int(filters*2)\n",
    "        layers_dict['Layer_' + str(layer)]['Pooling']['Encoding'] = lc.convolution_layer(filters, strides=(2,2))\n",
    "        layers_dict['Layer_' + str(layer)]['Decoding'] = []\n",
    "        encoding = []\n",
    "        for i in range(num_conv_blocks):\n",
    "            encoding += [block(filters)]\n",
    "        if layer == 0:\n",
    "            final_filters = filters\n",
    "        layers_dict['Layer_' + str(layer)]['Decoding'] = encoding\n",
    "    encoding = []\n",
    "    for i in range(num_conv_blocks):\n",
    "        encoding += [block(filters)]\n",
    "    layers_dict['Base'] = encoding\n",
    "    final_steps = [lc.convolution_layer(32, batch_norm=True, kernel=(1,1,1),activation='elu'),\n",
    "                   lc.convolution_layer(num_classes, batch_norm=False, activation='softmax')]\n",
    "    layers_dict['Final_Steps'] = final_steps\n",
    "    return layers_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "layers_dict = {}\n",
    "lc = Return_Layer_Functions(kernel=(3,3),strides=(1,1),padding='same',batch_norm=True,\n",
    "                                pooling_type='Max', pool_size=(2,2), bn_before_activation=False)\n",
    "conv_block = lc.convolution_layer\n",
    "layers_dict['Layer_0'] = {'Encoding': [conv_block(8),conv_block(8)],\n",
    "                          'Decoding': [conv_block(16), conv_block(16)],\n",
    "                          'Pooling':\n",
    "                              {'Encoding': [lc.convolution_layer(16, strides=(2, 2))],\n",
    "                               'Decoding': [lc.upsampling_layer(pool_size=(2, 2)),\n",
    "                                       lc.convolution_layer(16)]\n",
    "                               }}\n",
    "layers_dict['Base'] = [conv_block(16), conv_block(16)]\n",
    "layers_dict['Final_Steps'] =  [lc.convolution_layer(16, batch_norm=True, kernel=(1, 1),activation='elu'),\n",
    "                               lc.convolution_layer(2, batch_norm=False, activation='softmax')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "layers_dict['Layer_0'] = {'Encoding': [conv_block(16),activation,conv_block(16),activation],\n",
    "                          'Decoding': [conv_block(32),activation,conv_block(32),activation],\n",
    "                          'Pooling':\n",
    "                              {'Encoding': [pooling_downsampling],\n",
    "                               'Decoding': [pooling_upsampling]\n",
    "                               }}\n",
    "layers_dict['Base'] = [conv_block(32),activation,conv_block(32),activation]\n",
    "layers_dict['Final_Steps'] = [conv_block(2),{'activation':'softmax'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer_0\n",
      "Base\n",
      "Layer_0\n",
      "Final_Steps\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "gpu_options = GPUOptions(allow_growth=True)\n",
    "sess = Session(config=ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n",
    "set_session(sess)\n",
    "new_model = my_UNet(layers_dict=layers_dict, image_size=(128, 128, 3), is_2D=True).created_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name your model and define other things! Send a list of strings and it will make a folder path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "model_name = 'My_New_Model'\n",
    "model_path_out = os.path.join(model_path,model_name)\n",
    "if not os.path.exists(model_path_out):\n",
    "    os.makedirs(model_path_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets look at our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = TensorBoard(model_path_out)\n",
    "k.set_model(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir {\"./Models\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set a learning rate and loss metric, also add any metrics you want to track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_lr = 5e-6\n",
    "max_lr = 1e-3\n",
    "new_model.compile(tf.keras.optimizers.Adam(5e-5), loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "                  metrics=[tf.keras.metrics.CategoricalAccuracy(), MeanDSC(num_classes=2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is a checkpoint to save the model if it has the highest dice, also to add images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will specify that we want to watch the validation dice, and save the one with the highest value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor = 'val_mean_dsc'\n",
    "mode = 'max'\n",
    "checkpoint = ModelCheckpoint(os.path.join(model_path_out,'cp-best.ckpt'), monitor=monitor, verbose=1, save_best_only=True,\n",
    "                             save_weights_only=True, save_freq='epoch', mode=mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, our tensorboard output will add prediction images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CyclicLR will allow us to change the learning rate of the model as it runs, and Add_LR_To_Tensorboard will let us view it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = len(train_generator)//3\n",
    "step_size_factor = 10\n",
    "\n",
    "cyclic_lrate = CyclicLR(base_lr=min_lr, max_lr=max_lr, step_size=steps_per_epoch * step_size_factor, mode='triangular2')\n",
    "checkpoint = ModelCheckpoint(os.path.join(model_path_out,'cp-best.ckpt'), monitor=monitor, verbose=1, save_best_only=True,\n",
    "                              save_weights_only=False, mode='max')\n",
    "\n",
    "add_images = Add_Images_and_LR(log_dir=model_path_out, validation_data=validation_generator.data_set,\n",
    "                               number_of_images=len(validation_generator), add_images=True, image_frequency=5,\n",
    "                               threshold_x=True, target_image_height=128, target_image_width=128)\n",
    "tensorboard = TensorBoard(log_dir=model_path_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [cyclic_lrate, checkpoint, tensorboard, add_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "  2/496 [..............................] - ETA: 2:19 - loss: 1.1774 - categorical_accuracy: 0.3217 - mean_dsc: 0.0416WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.275017). Check your callbacks.\n",
      "494/496 [============================>.] - ETA: 0s - loss: 0.7898 - categorical_accuracy: 0.6500 - mean_dsc: 0.2064- ETA: 1s - loss: 0.8277 - categorical_accur\n",
      "Epoch 00001: val_mean_dsc improved from -inf to 0.30857, saving model to .\\..\\..\\Models\\My_New_Model\\cp-best.ckpt\n",
      "INFO:tensorflow:Assets written to: .\\..\\..\\Models\\My_New_Model\\cp-best.ckpt\\assets\n",
      "Writing out images\n",
      "Writing out image 0\n",
      "Writing out image 1\n",
      "Writing out image 2\n",
      "496/496 [==============================] - 22s 44ms/step - loss: 0.7892 - categorical_accuracy: 0.6505 - mean_dsc: 0.2068 - val_loss: 0.6190 - val_categorical_accuracy: 0.7993 - val_mean_dsc: 0.3086\n",
      "Epoch 2/5\n",
      "493/496 [============================>.] - ETA: 0s - loss: 0.5501 - categorical_accuracy: 0.8435 - mean_dsc: 0.4142- ETA: 0s - loss: 0.5534 - categorical_accuracy: 0.8415 - mean_dsc: 0.\n",
      "Epoch 00002: val_mean_dsc improved from 0.30857 to 0.52687, saving model to .\\..\\..\\Models\\My_New_Model\\cp-best.ckpt\n",
      "INFO:tensorflow:Assets written to: .\\..\\..\\Models\\My_New_Model\\cp-best.ckpt\\assets\n",
      "496/496 [==============================] - 26s 52ms/step - loss: 0.5496 - categorical_accuracy: 0.8438 - mean_dsc: 0.4141 - val_loss: 0.4042 - val_categorical_accuracy: 0.9266 - val_mean_dsc: 0.5269\n",
      "Epoch 3/5\n",
      "  1/496 [..............................] - ETA: 0s - loss: 0.4453 - categorical_accuracy: 0.8939 - mean_dsc: 0.4911"
     ]
    }
   ],
   "source": [
    "new_model.fit(train_generator.data_set, epochs=5, callbacks=callbacks, steps_per_epoch=len(train_generator),\n",
    "              validation_data=validation_generator.data_set, validation_steps=len(validation_generator),\n",
    "              validation_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir {\"./Models\"}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
