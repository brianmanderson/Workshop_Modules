{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Liver Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import some things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def return_sys_path():\n",
    "    path = '.'\n",
    "    for _ in range(5):\n",
    "        if 'Deep_Learning' in os.listdir(path):\n",
    "            break\n",
    "        else:\n",
    "            path = os.path.join(path,'..')\n",
    "    return path\n",
    "def return_data_path():\n",
    "    path = '.'\n",
    "    for _ in range(5):\n",
    "        if 'Data' in os.listdir(path):\n",
    "            break\n",
    "        else:\n",
    "            path = os.path.join(path,'..')\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(return_sys_path())\n",
    "from Deep_Learning.Base_Deeplearning_Code.Data_Generators.TFRecord_to_Dataset_Generator import *\n",
    "from Deep_Learning.Base_Deeplearning_Code.Callbacks.Visualizing_Model_Utils import TensorBoardImage\n",
    "from Deep_Learning.Base_Deeplearning_Code.Callbacks.TF2_Callbacks import Add_Images_and_LR\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
    "from Deep_Learning.Base_Deeplearning_Code.Plot_And_Scroll_Images.Plot_Scroll_Images import plot_Image_Scroll_Bar_Image\n",
    "import tensorflow as tf\n",
    "import tensorflow.python.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "base = return_sys_path()\n",
    "data_path = os.path.join(return_data_path(),'Data','Niftii_Arrays','Records')\n",
    "train_path = [os.path.join(data_path,'Train')]\n",
    "validation_path = os.path.join(data_path,'Validation')\n",
    "test_path = os.path.join(data_path,'Test')\n",
    "model_path = os.path.join(base,'Models')\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We now need some image processors...\n",
    "\n",
    "#### We will ensure that the images are 256 x 256 (downsampled for speed), normalize them with a mean of 78 and std of 29, add random noise, threshold, and turn into 2 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "image_size = 128\n",
    "wanted_keys={'inputs':['image','mask'],'outputs':['annotation']}\n",
    "image_processors_train = [Ensure_Image_Proportions(image_size,image_size),Repeat_Channel(repeats=3),\n",
    "                          Normalize_Images(mean_val=78,std_val=29),\n",
    "                          Threshold_Images(lower_bound=-3.55,upper_bound=3.55),\n",
    "                          Return_Outputs(wanted_keys)]\n",
    "#Add_Noise_To_Images(variation=np.round(np.arange(start=0, stop=0.3, step=0.1),2)),\n",
    "image_processors_test = [Ensure_Image_Proportions(image_size,image_size),Normalize_Images(mean_val=78,std_val=29),\n",
    "                         Repeat_Channel(repeats=3), \n",
    "                         Threshold_Images(lower_bound=-3.55,upper_bound=3.55)\n",
    "                         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-d0675e7790fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_generator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mData_Generator_Class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord_paths\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m image_processors_train += [\n\u001b[0;32m      5\u001b[0m             {'shuffle': len(train_generator)}, {'batch': batch_size}, {'repeat'}]\n",
      "\u001b[1;32mH:\\Modular_Projects\\MD_Anderson_Intro_To_AI_Workshop\\Deep_Learning\\Base_Deeplearning_Code\\Data_Generators\\TFRecord_to_Dataset_Generator.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, record_paths, in_parallel, delete_old_cache, shuffle)\u001b[0m\n\u001b[0;32m     60\u001b[0m                     \u001b[0mfid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_examples\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m             \u001b[0mparsed_image_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mraw_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreturn_parse_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m             \u001b[0mDecode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecode_Images_Annotations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_type_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0md_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[0mdecoded_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsed_image_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDecode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_parallel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\bmanderson\\virtual_environments\\tensorflow2.2\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[0;32m   1619\u001b[0m     \"\"\"\n\u001b[0;32m   1620\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1621\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1622\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1623\u001b[0m       return ParallelMapDataset(\n",
      "\u001b[1;32mc:\\users\\bmanderson\\virtual_environments\\tensorflow2.2\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[0;32m   3979\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3980\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3981\u001b[1;33m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[0;32m   3982\u001b[0m     variant_tensor = gen_dataset_ops.map_dataset(\n\u001b[0;32m   3983\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\bmanderson\\virtual_environments\\tensorflow2.2\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m   3219\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3220\u001b[0m         \u001b[1;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3221\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3223\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\bmanderson\\virtual_environments\\tensorflow2.2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2530\u001b[0m     \"\"\"\n\u001b[0;32m   2531\u001b[0m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[1;32m-> 2532\u001b[1;33m         *args, **kwargs)\n\u001b[0m\u001b[0;32m   2533\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2534\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\bmanderson\\virtual_environments\\tensorflow2.2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2494\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2495\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2496\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2497\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2498\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\bmanderson\\virtual_environments\\tensorflow2.2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2775\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2777\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2779\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\bmanderson\\virtual_environments\\tensorflow2.2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   2665\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2666\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2667\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   2668\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2669\u001b[0m         \u001b[1;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\bmanderson\\virtual_environments\\tensorflow2.2\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 981\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    982\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    983\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\bmanderson\\virtual_environments\\tensorflow2.2\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   3212\u001b[0m           attributes=defun_kwargs)\n\u001b[0;32m   3213\u001b[0m       \u001b[1;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=missing-docstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3214\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3215\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3216\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\bmanderson\\virtual_environments\\tensorflow2.2\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m_wrapper_helper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   3154\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3156\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3157\u001b[0m       \u001b[1;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3158\u001b[0m       \u001b[1;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\bmanderson\\virtual_environments\\tensorflow2.2\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m           \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m           \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    H:\\Modular_Projects\\MD_Anderson_Intro_To_AI_Workshop\\Deep_Learning\\Base_Deeplearning_Code\\Data_Generators\\TFRecord_to_Dataset_Generator.py:23 _parse_image_function  *\n        return tf.io.parse_single_example(example_proto, image_feature_description)\n    c:\\users\\bmanderson\\virtual_environments\\tensorflow2.2\\lib\\site-packages\\tensorflow\\python\\ops\\parsing_ops.py:443 parse_single_example_v2  **\n        raise ValueError(\"Missing features.\")\n\n    ValueError: Missing features.\n"
     ],
     "ename": "ValueError",
     "evalue": "in user code:\n\n    H:\\Modular_Projects\\MD_Anderson_Intro_To_AI_Workshop\\Deep_Learning\\Base_Deeplearning_Code\\Data_Generators\\TFRecord_to_Dataset_Generator.py:23 _parse_image_function  *\n        return tf.io.parse_single_example(example_proto, image_feature_description)\n    c:\\users\\bmanderson\\virtual_environments\\tensorflow2.2\\lib\\site-packages\\tensorflow\\python\\ops\\parsing_ops.py:443 parse_single_example_v2  **\n        raise ValueError(\"Missing features.\")\n\n    ValueError: Missing features.\n",
     "output_type": "error"
    }
   ],
   "source": [
    "batch_size = 5\n",
    "train_generator = Data_Generator_Class(record_paths=train_path)\n",
    "\n",
    "image_processors_train += [\n",
    "            {'shuffle': len(train_generator)}, {'batch': batch_size}, {'repeat'}]\n",
    "train_generator.compile_data_set(image_processors_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets visualize one of the examples! With batch_size of 5 and shuffle on, it will be 5 random 2D slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = train_generator.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_Image_Scroll_Bar_Image(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alright, lets make our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Deep_Learning.Easy_VGG16_UNet.Keras_Fine_Tune_VGG_16_Liver import VGG_16\n",
    "from Deep_Learning.Base_Deeplearning_Code.Visualizing_Model.Visualing_Model import visualization_model_class\n",
    "from tensorflow.python.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is just a click and play, it builds the VGG16 architecture for you with pre-trained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![VGG16_Unet.png](./Deep_Learning/Easy_VGG16_UNet/VGG16_UNet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n",
    "K.set_session(sess)\n",
    "network = {'Layer_0': {'Encoding': [64, 64], 'Decoding': [64]},\n",
    "           'Layer_1': {'Encoding': [128, 128], 'Decoding': [64]},\n",
    "           'Layer_2': {'Encoding': [256, 256, 256], 'Decoding': [256]},\n",
    "           'Layer_3': {'Encoding': [512, 512, 512], 'Decoding': [256]},\n",
    "           'Layer_4': {'Encoding': [512, 512, 512]}}\n",
    "VGG_model = VGG_16(network=network, activation='relu',filter_size=(3,3))\n",
    "VGG_model.make_model()\n",
    "VGG_model.load_weights()\n",
    "new_model = VGG_model.created_model\n",
    "model_path = os.path.join(return_sys_path(),'Models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## These are some tools for visualizing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Visualizing_Class = visualization_model_class(model=new_model, save_images=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets look at the activations of block1_conv1, the activation, and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualizing_Class.define_desired_layers(['block1_conv1','block1_conv1_activation','Output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualizing_Class.predict_on_tensor(x[0,...][None,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Visualizing_Class.plot_activations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_model.compile(Adam(lr=5e-5),loss='categorical_crossentropy', metrics=['accuracy',dice_coef_3D])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freezing pre-trained layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_until_name(model,name):\n",
    "    set_trainable = False\n",
    "    for layer in model.layers:\n",
    "        if layer.name == name:\n",
    "            set_trainable = True\n",
    "        layer.trainable = set_trainable\n",
    "    return model\n",
    "new_model = freeze_until_name(new_model,'Upsampling0_UNet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint and run\n",
    "\n",
    "A checkpoint is a way of assessing the model and determining if we should save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'VGG_16_Model'\n",
    "model_path_out = os.path.join(model_path,'VGG_16_frozen')\n",
    "if not os.path.exists(model_path_out):\n",
    "    os.makedirs(model_path_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(os.path.join(model_path_out,'best-model.hdf5'), monitor='val_dice_coef_3D', verbose=1, save_best_only=True,\n",
    "                              save_weights_only=False, mode='max')\n",
    "# TensorboardImage lets us view the predictions of our model\n",
    "tensorboard = TensorBoardImage(log_dir=model_path_out, num_images=3,update_freq='epoch', \n",
    "                               data_generator=validation_generator, image_frequency=1)\n",
    "callbacks = [checkpoint, tensorboard]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets view the model real quick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.callbacks_v1 import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = TensorBoard(model_path_out)\n",
    "k.set_model(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir {\"./Models\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.fit_generator(train_generator,epochs=5, workers=20, max_queue_size=50,\n",
    "                        validation_data=validation_generator,callbacks=callbacks,\n",
    "                        steps_per_epoch=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = validation_generator.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = new_model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[pred<0.5] = 0\n",
    "pred[pred>0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scroll_Image(pred[...,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now lets make our own architecture\n",
    "\n",
    "### First, lets import some necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Deep_Learning.Base_Deeplearning_Code.Models.Keras_Models import my_UNet\n",
    "from Deep_Learning.Base_Deeplearning_Code.Cyclical_Learning_Rate.clr_callback import CyclicLR\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
    "from functools import partial\n",
    "from tensorflow.python.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our convolution and strided blocks, strided is used for downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = {'activation': 'relu'}\n",
    "kernel = (3,3)\n",
    "pool_size = (2,2)\n",
    "#{'channels': x, 'kernel': (3, 3), 'strides': (1, 1),'activation':activation}\n",
    "conv_block = lambda x: {'convolution': {'channels': x, 'kernel': (3, 3),\n",
    "                                        'activation': None, 'strides': (1, 1)}}\n",
    "pooling_downsampling = {'pooling': {'pooling_type': 'Max',\n",
    "                                    'pool_size': (2, 2), 'direction': 'Down'}}\n",
    "pooling_upsampling = {'pooling': {'pool_size': (2, 2), 'direction': 'Up'}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our architecture will have 2 main parts in each layer, an 'Encoding' and a 'Decoding' side, 'Encoding' goes down, and 'Decoding' goes up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Encoding and Decoding.png](./Deep_Learning/Encoding_and_Decoding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will now create our layer dictionary, this tells our UNet what to look like\n",
    "\n",
    "### If Pooling is left {} it will perform maxpooling and upsampling with pooling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dict = {}\n",
    "layers_dict['Layer_0'] = {'Encoding': [],\n",
    "                          'Decoding': [],\n",
    "                          'Pooling':\n",
    "                              {'Encoding': [],\n",
    "                               'Decoding': []\n",
    "                               }}\n",
    "layers_dict['Base'] = []\n",
    "layers_dict['Final_Steps'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dict['Layer_0'] = {'Encoding': [conv_block(16),activation,conv_block(16),activation],\n",
    "                          'Decoding': [conv_block(32),activation,conv_block(32),activation],\n",
    "                          'Pooling':\n",
    "                              {'Encoding': [pooling_downsampling],\n",
    "                               'Decoding': [pooling_upsampling]\n",
    "                               }}\n",
    "layers_dict['Base'] = [conv_block(32),activation,conv_block(32),activation]\n",
    "layers_dict['Final_Steps'] = [conv_block(2),{'activation':'softmax'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n",
    "K.set_session(sess)\n",
    "new_model = my_UNet(kernel=(3, 3), layers_dict=layers_dict, pool_size=(2, 2), input_size=3, image_size=image_size).created_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name your model and define other things! Send a list of strings and it will make a folder path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'My_New_Model'\n",
    "model_path_out = os.path.join(model_path,model_name)\n",
    "if not os.path.exists(model_path_out):\n",
    "    os.makedirs(model_path_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets look at our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.callbacks_v1 import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = TensorBoard(model_path_out)\n",
    "k.set_model(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir {\"./Models\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set a learning rate and loss metric, also add any metrics you want to track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_lr = 5e-6\n",
    "max_lr = 1e-3\n",
    "new_model.compile(Adam(lr=min_lr),loss='categorical_crossentropy', metrics=['accuracy',dice_coef_3D])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is a checkpoint to save the model if it has the highest dice, also to add images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will specify that we want to watch the validation dice, and save the one with the highest value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor = 'val_dice_coef_3D'\n",
    "mode = 'max'\n",
    "checkpoint = ModelCheckpoint(os.path.join(model_path_out,'best-model.hdf5'), monitor=monitor, verbose=1, save_best_only=True,\n",
    "                             save_weights_only=False, save_freq='epoch', mode=mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, our tensorboard output will add prediction images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorboardImage lets us view the predictions of our model\n",
    "tensorboard = TensorBoardImage(log_dir=model_path_out, num_images=1,update_freq='epoch',\n",
    "                               data_generator=validation_generator, image_frequency=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CyclicLR will allow us to change the learning rate of the model as it runs, and Add_LR_To_Tensorboard will let us view it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = len(train_generator)//3\n",
    "step_size_factor = 10\n",
    "\n",
    "cyclic_lrate = CyclicLR(base_lr=min_lr, max_lr=max_lr, step_size=steps_per_epoch * step_size_factor, mode='triangular2')\n",
    "add_lr_to_tensorboard = Add_LR_To_Tensorboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [cyclic_lrate, add_lr_to_tensorboard, tensorboard, checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.fit_generator(train_generator,epochs=10, workers=10, max_queue_size=200, validation_data=validation_generator,\n",
    "                       callbacks=callbacks, steps_per_epoch=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir {\"./Models\"}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}