{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Liver Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import some things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def return_sys_path():\n",
    "    path = '.'\n",
    "    for _ in range(5):\n",
    "        if 'Deep_Learning' in os.listdir(path):\n",
    "            break\n",
    "        else:\n",
    "            path = os.path.join(path,'..')\n",
    "    return path\n",
    "def return_data_path():\n",
    "    path = '.'\n",
    "    for _ in range(5):\n",
    "        if 'Data' in os.listdir(path):\n",
    "            break\n",
    "        else:\n",
    "            path = os.path.join(path,'..')\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(return_sys_path())\n",
    "from Deep_Learning.Base_Deeplearning_Code.Data_Generators.Generators import Data_Generator_Class, os\n",
    "from Deep_Learning.Base_Deeplearning_Code.Keras_Utils.Keras_Utilities import np, dice_coef_3D\n",
    "from Deep_Learning.Base_Deeplearning_Code.Data_Generators.Image_Processors import *\n",
    "from Deep_Learning.Base_Deeplearning_Code.Callbacks.Visualizing_Model_Utils import TensorBoardImage\n",
    "from Deep_Learning.Base_Deeplearning_Code.Callbacks.BMA_Callbacks import Add_LR_To_Tensorboard\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
    "# from Utils import ModelCheckpoint, model_path_maker\n",
    "from Deep_Learning.Base_Deeplearning_Code.Plot_And_Scroll_Images.Plot_Scroll_Images import plot_Image_Scroll_Bar_Image\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow.python.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = return_sys_path()\n",
    "data_path = os.path.join(return_data_path(),'Data','Niftii_Arrays')\n",
    "train_path = [os.path.join(data_path,'Train')]\n",
    "validation_path = os.path.join(data_path,'Validation')\n",
    "test_path = os.path.join(data_path,'Test')\n",
    "model_path = os.path.join(base,'Models')\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We now need some image processors...\n",
    "\n",
    "#### We will ensure that the images are 256 x 256 (downsampled for speed), normalize them with a mean of 78 and std of 29, add random noise, threshold, and turn into 2 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 128\n",
    "image_processors_train = [Ensure_Image_Proportions(image_size,image_size),Repeat_Channel(repeats=3),\n",
    "                          Normalize_Images(mean_val=78,std_val=29),\n",
    "                          Threshold_Images(lower_bound=-3.55,upper_bound=3.55, post_load=False, final_scale_value=1),\n",
    "                          Annotations_To_Categorical(2)]\n",
    "#Add_Noise_To_Images(variation=np.round(np.arange(start=0, stop=0.3, step=0.1),2)),\n",
    "image_processors_test = [Ensure_Image_Proportions(image_size,image_size),Normalize_Images(mean_val=78,std_val=29),\n",
    "                         Repeat_Channel(repeats=3), \n",
    "                         Threshold_Images(lower_bound=-3.55,upper_bound=3.55, post_load=False,final_scale_value=1),\n",
    "                         Annotations_To_Categorical(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "train_generator = Data_Generator_Class(by_patient=False,shuffle=True,data_paths=train_path,batch_size=batch_size,\n",
    "                                       image_processors=image_processors_train)\n",
    "validation_generator = Data_Generator_Class(by_patient=True,shuffle=True,data_paths=validation_path,batch_size=30,\n",
    "                                            image_processors=image_processors_test, by_patient_2D=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets visualize one of the examples! With batch_size of 5 and shuffle on, it will be 5 random 2D slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = train_generator.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_Image_Scroll_Bar_Image(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alright, lets make our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Deep_Learning.Easy_VGG16_UNet.Keras_Fine_Tune_VGG_16_Liver import VGG_16\n",
    "from Deep_Learning.Base_Deeplearning_Code.Visualizing_Model.Visualing_Model import visualization_model_class\n",
    "from tensorflow.python.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is just a click and play, it builds the VGG16 architecture for you with pre-trained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![VGG16_Unet.png](./Deep_Learning/Easy_VGG16_UNet/VGG16_UNet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n",
    "K.set_session(sess)\n",
    "network = {'Layer_0': {'Encoding': [64, 64], 'Decoding': [64]},\n",
    "           'Layer_1': {'Encoding': [128, 128], 'Decoding': [64]},\n",
    "           'Layer_2': {'Encoding': [256, 256, 256], 'Decoding': [256]},\n",
    "           'Layer_3': {'Encoding': [512, 512, 512], 'Decoding': [256]},\n",
    "           'Layer_4': {'Encoding': [512, 512, 512]}}\n",
    "VGG_model = VGG_16(network=network, activation='relu',filter_size=(3,3))\n",
    "VGG_model.make_model()\n",
    "VGG_model.load_weights()\n",
    "new_model = VGG_model.created_model\n",
    "model_path = os.path.join(return_sys_path(),'Models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## These are some tools for visualizing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Visualizing_Class = visualization_model_class(model=new_model, save_images=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets look at the activations of block1_conv1, the activation, and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualizing_Class.define_desired_layers(['block1_conv1','block1_conv1_activation','Output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualizing_Class.predict_on_tensor(x[0,...][None,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Visualizing_Class.plot_activations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_model.compile(Adam(lr=5e-5),loss='categorical_crossentropy', metrics=['accuracy',dice_coef_3D])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freezing pre-trained layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_until_name(model,name):\n",
    "    set_trainable = False\n",
    "    for layer in model.layers:\n",
    "        if layer.name == name:\n",
    "            set_trainable = True\n",
    "        layer.trainable = set_trainable\n",
    "    return model\n",
    "new_model = freeze_until_name(new_model,'Upsampling0_UNet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint and run\n",
    "\n",
    "A checkpoint is a way of assessing the model and determining if we should save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'VGG_16_Model'\n",
    "model_path_out = os.path.join(model_path,'VGG_16_frozen')\n",
    "if not os.path.exists(model_path_out):\n",
    "    os.makedirs(model_path_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(os.path.join(model_path_out,'best-model.hdf5'), monitor='val_dice_coef_3D', verbose=1, save_best_only=True,\n",
    "                              save_weights_only=False, mode='max')\n",
    "# TensorboardImage lets us view the predictions of our model\n",
    "tensorboard = TensorBoardImage(log_dir=model_path_out, num_images=3,update_freq='epoch', \n",
    "                               data_generator=validation_generator, image_frequency=1)\n",
    "callbacks = [checkpoint, tensorboard]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets view the model real quick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.callbacks_v1 import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = TensorBoard(model_path_out)\n",
    "k.set_model(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir {\"./Models\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.fit_generator(train_generator,epochs=5, workers=20, max_queue_size=50,\n",
    "                        validation_data=validation_generator,callbacks=callbacks,\n",
    "                        steps_per_epoch=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = validation_generator.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = new_model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[pred<0.5] = 0\n",
    "pred[pred>0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scroll_Image(pred[...,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now lets make our own architecture\n",
    "\n",
    "### First, lets import some necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Deep_Learning.Base_Deeplearning_Code.Models.Keras_Models import my_UNet\n",
    "from Deep_Learning.Base_Deeplearning_Code.Cyclical_Learning_Rate.clr_callback import CyclicLR\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
    "from functools import partial\n",
    "from tensorflow.python.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our convolution and strided blocks, strided is used for downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = {'activation': 'relu'}\n",
    "kernel = (3,3)\n",
    "pool_size = (2,2)\n",
    "#{'channels': x, 'kernel': (3, 3), 'strides': (1, 1),'activation':activation}\n",
    "conv_block = lambda x: {'convolution': {'channels': x, 'kernel': (3, 3),\n",
    "                                        'activation': None, 'strides': (1, 1)}}\n",
    "pooling_downsampling = {'pooling': {'pooling_type': 'Max',\n",
    "                                    'pool_size': (2, 2), 'direction': 'Down'}}\n",
    "pooling_upsampling = {'pooling': {'pool_size': (2, 2), 'direction': 'Up'}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our architecture will have 2 main parts in each layer, an 'Encoding' and a 'Decoding' side, 'Encoding' goes down, and 'Decoding' goes up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Encoding and Decoding.png](./Deep_Learning/Encoding_and_Decoding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will now create our layer dictionary, this tells our UNet what to look like\n",
    "\n",
    "### If Pooling is left {} it will perform maxpooling and upsampling with pooling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dict = {}\n",
    "layers_dict['Layer_0'] = {'Encoding': [],\n",
    "                          'Decoding': [],\n",
    "                          'Pooling':\n",
    "                              {'Encoding': [],\n",
    "                               'Decoding': []\n",
    "                               }}\n",
    "layers_dict['Base'] = []\n",
    "layers_dict['Final_Steps'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dict['Layer_0'] = {'Encoding': [conv_block(16),activation,conv_block(16),activation],\n",
    "                          'Decoding': [conv_block(32),activation,conv_block(32),activation],\n",
    "                          'Pooling':\n",
    "                              {'Encoding': [pooling_downsampling],\n",
    "                               'Decoding': [pooling_upsampling]\n",
    "                               }}\n",
    "layers_dict['Base'] = [conv_block(32),activation,conv_block(32),activation]\n",
    "layers_dict['Final_Steps'] = [conv_block(2),{'activation':'softmax'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n",
    "K.set_session(sess)\n",
    "new_model = my_UNet(kernel=(3, 3), layers_dict=layers_dict, pool_size=(2, 2), input_size=3, image_size=image_size).created_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name your model and define other things! Send a list of strings and it will make a folder path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'My_New_Model'\n",
    "model_path_out = os.path.join(model_path,model_name)\n",
    "if not os.path.exists(model_path_out):\n",
    "    os.makedirs(model_path_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets look at our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.callbacks_v1 import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = TensorBoard(model_path_out)\n",
    "k.set_model(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir {\"./Models\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set a learning rate and loss metric, also add any metrics you want to track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_lr = 5e-6\n",
    "max_lr = 1e-3\n",
    "new_model.compile(Adam(lr=min_lr),loss='categorical_crossentropy', metrics=['accuracy',dice_coef_3D])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is a checkpoint to save the model if it has the highest dice, also to add images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will specify that we want to watch the validation dice, and save the one with the highest value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor = 'val_dice_coef_3D'\n",
    "mode = 'max'\n",
    "checkpoint = ModelCheckpoint(os.path.join(model_path_out,'best-model.hdf5'), monitor=monitor, verbose=1, save_best_only=True,\n",
    "                             save_weights_only=False, save_freq='epoch', mode=mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, our tensorboard output will add prediction images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorboardImage lets us view the predictions of our model\n",
    "tensorboard = TensorBoardImage(log_dir=model_path_out, num_images=1,update_freq='epoch',\n",
    "                               data_generator=validation_generator, image_frequency=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CyclicLR will allow us to change the learning rate of the model as it runs, and Add_LR_To_Tensorboard will let us view it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = len(train_generator)//3\n",
    "step_size_factor = 10\n",
    "\n",
    "cyclic_lrate = CyclicLR(base_lr=min_lr, max_lr=max_lr, step_size=steps_per_epoch * step_size_factor, mode='triangular2')\n",
    "add_lr_to_tensorboard = Add_LR_To_Tensorboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [cyclic_lrate, add_lr_to_tensorboard, tensorboard, checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.fit_generator(train_generator,epochs=10, workers=10, max_queue_size=200, validation_data=validation_generator,\n",
    "                       callbacks=callbacks, steps_per_epoch=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir {\"./Models\"}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}